# Langfuse SDK v3 é›†æˆæŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨ Langfuse SDK v3 æ¥è¿½è¸ªå’Œè§‚æµ‹ LLM åº”ç”¨ã€‚Langfuse SDK v3 æ˜¯åŸºäº OpenTelemetry åŸç”Ÿæ„å»ºçš„ï¼Œæä¾›äº†å¼€ç®±å³ç”¨çš„ LLM å¯è§‚æµ‹æ€§åŠŸèƒ½ã€‚

> ğŸ“Œ æœ¬æŒ‡å—åŒæ—¶æ¶µç›– **Python** å’Œ **JavaScript/TypeScript** ä¸¤ç§è¯­è¨€çš„ä½¿ç”¨æ–¹å¼ã€‚

## ç›®å½•

- [1. å®‰è£…ä¸é…ç½®](#1-å®‰è£…ä¸é…ç½®)
- [2. æ ¸å¿ƒæ¦‚å¿µ](#2-æ ¸å¿ƒæ¦‚å¿µ)
- [3. åŸ‹ç‚¹æ–¹å¼](#3-åŸ‹ç‚¹æ–¹å¼)
  - [3.1 è£…é¥°å™¨/åŒ…è£…å™¨æ¨¡å¼](#31-è£…é¥°å™¨åŒ…è£…å™¨æ¨¡å¼)
  - [3.2 ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¨¡å¼](#32-ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¨¡å¼)
  - [3.3 æ‰‹åŠ¨åˆ›å»ºè§‚æµ‹](#33-æ‰‹åŠ¨åˆ›å»ºè§‚æµ‹)
- [4. åµŒå¥—è¿½è¸ª](#4-åµŒå¥—è¿½è¸ª)
- [5. å±æ€§ä¸å…ƒæ•°æ®](#5-å±æ€§ä¸å…ƒæ•°æ®)
- [6. ä¸ OpenAI é›†æˆ](#6-ä¸-openai-é›†æˆ)
- [7. ä¸ LangChain é›†æˆ](#7-ä¸-langchain-é›†æˆ)
- [8. è¯„åˆ†ä¸åé¦ˆ](#8-è¯„åˆ†ä¸åé¦ˆ)
- [9. å®Œæ•´ç¤ºä¾‹](#9-å®Œæ•´ç¤ºä¾‹)
- [10. å¸¸è§é—®é¢˜](#10-å¸¸è§é—®é¢˜)

---

## 1. å®‰è£…ä¸é…ç½®

### Python å®‰è£…

```bash
pip install langfuse
```

### JavaScript/TypeScript å®‰è£…

```bash
npm install @langfuse/tracing @langfuse/otel @opentelemetry/sdk-node
# å¯é€‰åŒ…
npm install @langfuse/client   # API å®¢æˆ·ç«¯
npm install @langfuse/openai   # OpenAI è‡ªåŠ¨åŸ‹ç‚¹
npm install @langfuse/langchain # LangChain é›†æˆ
```

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# å¿…éœ€é…ç½®
export LANGFUSE_PUBLIC_KEY="pk-lf-xxx"
export LANGFUSE_SECRET_KEY="sk-lf-xxx"

# å¯é€‰é…ç½®ï¼ˆé»˜è®¤ä¸º Langfuse Cloud EUï¼‰
export LANGFUSE_BASE_URL="https://cloud.langfuse.com"  # EU åŒºåŸŸï¼ˆé»˜è®¤ï¼‰
# export LANGFUSE_BASE_URL="https://us.cloud.langfuse.com"  # US åŒºåŸŸ
# export LANGFUSE_BASE_URL="http://localhost:3000"  # æœ¬åœ°éƒ¨ç½²
```

### ä»£ç ä¸­é…ç½®

<details>
<summary>ğŸ Python</summary>

```python
from langfuse import Langfuse

# æ–¹å¼1ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰
langfuse = Langfuse()

# æ–¹å¼2ï¼šä»£ç ä¸­ç›´æ¥é…ç½®
langfuse = Langfuse(
    public_key="pk-lf-xxx",
    secret_key="sk-lf-xxx",
    host="https://cloud.langfuse.com"
)
```
</details>

<details>
<summary>ğŸ“˜ JavaScript/TypeScript</summary>

**Step 1: åˆ›å»º instrumentation.tsï¼ˆåˆå§‹åŒ– OpenTelemetryï¼‰**

```typescript
// instrumentation.ts
import { NodeSDK } from "@opentelemetry/sdk-node";
import { LangfuseSpanProcessor } from "@langfuse/otel";

export const sdk = new NodeSDK({
  spanProcessors: [new LangfuseSpanProcessor()],
});

sdk.start();
```

**Step 2: åˆå§‹åŒ–å®¢æˆ·ç«¯**

```typescript
import { LangfuseClient } from "@langfuse/client";

// æ–¹å¼1ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰
const langfuse = new LangfuseClient();

// æ–¹å¼2ï¼šä»£ç ä¸­ç›´æ¥é…ç½®
const langfuse = new LangfuseClient({
  publicKey: "pk-lf-xxx",
  secretKey: "sk-lf-xxx",
  baseUrl: "https://cloud.langfuse.com",
});
```
</details>

---

## 2. æ ¸å¿ƒæ¦‚å¿µ

### Traceï¼ˆè¿½è¸ªï¼‰
ä»£è¡¨ä¸€æ¬¡å®Œæ•´çš„è¯·æ±‚æˆ–äº¤äº’ï¼ŒåŒ…å«å¤šä¸ª Observationã€‚

### Observationï¼ˆè§‚æµ‹ï¼‰
Trace ä¸­çš„å•ä¸ªæ­¥éª¤ï¼Œæœ‰ä»¥ä¸‹ç±»å‹ï¼š

| ç±»å‹ | è¯´æ˜ | å…¸å‹ç”¨é€” |
|------|------|----------|
| **span** | é€šç”¨æ­¥éª¤ | å‡½æ•°è°ƒç”¨ã€å¤„ç†æµç¨‹ |
| **generation** | LLM ç”Ÿæˆ | GPT/Claude ç­‰æ¨¡å‹è°ƒç”¨ |
| **event** | äº‹ä»¶ | ç”¨æˆ·ç‚¹å‡»ã€ç³»ç»Ÿäº‹ä»¶ |

### å±‚çº§å…³ç³»

```
Trace
â”œâ”€â”€ Span: "user-request-pipeline"
â”‚   â”œâ”€â”€ Generation: "gpt-4-call"
â”‚   â”œâ”€â”€ Span: "data-processing"
â”‚   â””â”€â”€ Generation: "summarization"
â””â”€â”€ Event: "user-feedback"
```

---

## 3. åŸ‹ç‚¹æ–¹å¼

Langfuse SDK v3 æä¾›ä¸‰ç§åŸ‹ç‚¹æ–¹å¼ï¼Œé€‚ç”¨äºä¸åŒåœºæ™¯ã€‚

### 3.1 è£…é¥°å™¨/åŒ…è£…å™¨æ¨¡å¼

æœ€ç®€å•çš„æ–¹å¼ï¼Œè‡ªåŠ¨æ•è·å‡½æ•°çš„è¾“å…¥ã€è¾“å‡ºã€æ‰§è¡Œæ—¶é—´å’Œé”™è¯¯ã€‚

<details>
<summary>ğŸ Python - @observe è£…é¥°å™¨</summary>

#### åŸºç¡€ç”¨æ³•

```python
from langfuse import observe

@observe()
def process_user_query(query: str) -> dict:
    """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
    result = {"answer": f"å¤„ç†ç»“æœ: {query}"}
    return result

# è°ƒç”¨å‡½æ•°æ—¶è‡ªåŠ¨åˆ›å»º trace å’Œ span
result = process_user_query("ä»€ä¹ˆæ˜¯ Langfuse?")
```

#### æŒ‡å®šè§‚æµ‹ç±»å‹

```python
from langfuse import observe

# æ ‡è®°ä¸º generation ç±»å‹ï¼ˆç”¨äº LLM è°ƒç”¨ï¼‰
@observe(name="llm-generation", as_type="generation")
def call_llm(prompt: str) -> str:
    return f"LLM å“åº”: {prompt}"

# æ ‡è®°ä¸ºæ™®é€š span
@observe(name="data-processing", as_type="span")
def process_data(data: dict) -> dict:
    return {"processed": True, **data}
```

#### ç¦ç”¨è¾“å…¥/è¾“å‡ºæ•è·

```python
@observe(capture_input=False, capture_output=False)
def handle_large_data(large_data: bytes) -> bytes:
    return large_data
```
</details>

<details>
<summary>ğŸ“˜ JavaScript/TypeScript - observe åŒ…è£…å™¨</summary>

#### åŸºç¡€ç”¨æ³•

```typescript
import { observe, updateActiveObservation } from "@langfuse/tracing";

async function fetchData(source: string) {
  updateActiveObservation({ metadata: { source: "API" } });
  return { data: `some data from ${source}` };
}

// ä½¿ç”¨ observe åŒ…è£…å‡½æ•°
const tracedFetchData = observe(fetchData, {
  name: "fetch-data",
  asType: "span",
});

const result = await tracedFetchData("API");
```

#### æŒ‡å®šè§‚æµ‹ç±»å‹

```typescript
import { observe } from "@langfuse/tracing";

// æ ‡è®°ä¸º generation ç±»å‹
const tracedLLMCall = observe(
  async (prompt: string) => {
    return `LLM å“åº”: ${prompt}`;
  },
  { name: "llm-call", asType: "generation" }
);

// æ ‡è®°ä¸º span ç±»å‹
const tracedProcess = observe(
  (data: any) => ({ processed: true, ...data }),
  { name: "data-processing", asType: "span" }
);
```

#### ç¦ç”¨è¾“å…¥/è¾“å‡ºæ•è·

```typescript
const tracedFn = observe(myFunction, {
  name: "my-function",
  captureInput: false,
  captureOutput: false,
});
```
</details>

---

### 3.2 ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¨¡å¼

æä¾›æ›´çµæ´»çš„æ§åˆ¶ï¼Œé€‚åˆéœ€è¦æ‰‹åŠ¨æ›´æ–°è§‚æµ‹æ•°æ®çš„åœºæ™¯ã€‚

<details>
<summary>ğŸ Python</summary>

#### åŸºç¡€ç”¨æ³•

```python
from langfuse import get_client

langfuse = get_client()

with langfuse.start_as_current_observation(
    as_type="span",
    name="user-request-pipeline",
    input={"user_query": "å‘Šè¯‰æˆ‘ä¸€ä¸ªç¬‘è¯"}
) as span:
    # åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œçš„æ‰€æœ‰æ“ä½œéƒ½ä¼šè¢«è¿½è¸ª
    span.update(output={"joke": "ä¸ºä»€ä¹ˆç¨‹åºå‘˜æ€»æ˜¯å†·ï¼Ÿå› ä¸ºä»–ä»¬å–œæ¬¢ Windows..."})
```

#### åµŒå¥—åˆ›å»º generation

```python
from langfuse import get_client, propagate_attributes

langfuse = get_client()

with langfuse.start_as_current_observation(
    as_type="span",
    name="chat-workflow",
    input={"user_message": "ä½ å¥½"}
) as root_span:
    
    with propagate_attributes(user_id="user_123", session_id="session_abc"):
        
        with langfuse.start_as_current_observation(
            as_type="generation",
            name="gpt-4-call",
            model="gpt-4",
            input=[{"role": "user", "content": "ä½ å¥½"}]
        ) as generation:
            
            llm_response = "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"
            generation.update(
                output=llm_response,
                usage={"input_tokens": 10, "output_tokens": 15}
            )
    
    root_span.update(output={"response": llm_response})
```
</details>

<details>
<summary>ğŸ“˜ JavaScript/TypeScript</summary>

#### åŸºç¡€ç”¨æ³•

```typescript
import { startActiveObservation } from "@langfuse/tracing";

await startActiveObservation("user-request-pipeline", async (span) => {
  span.update({
    input: { user_query: "å‘Šè¯‰æˆ‘ä¸€ä¸ªç¬‘è¯" },
  });

  // æ‰§è¡Œä¸šåŠ¡é€»è¾‘
  const joke = "ä¸ºä»€ä¹ˆç¨‹åºå‘˜æ€»æ˜¯å†·ï¼Ÿå› ä¸ºä»–ä»¬å–œæ¬¢ Windows...";

  span.update({
    output: { joke },
  });
});
```

#### åµŒå¥—åˆ›å»º generation

```typescript
import {
  startActiveObservation,
  startObservation,
  propagateAttributes,
} from "@langfuse/tracing";

await startActiveObservation("chat-workflow", async (rootSpan) => {
  rootSpan.update({ input: { user_message: "ä½ å¥½" } });

  await propagateAttributes(
    {
      userId: "user_123",
      sessionId: "session_abc",
    },
    async () => {
      const generation = startObservation(
        "gpt-4-call",
        {
          model: "gpt-4",
          input: [{ role: "user", content: "ä½ å¥½" }],
        },
        { asType: "generation" }
      );

      const llmResponse = "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ";

      generation.update({
        output: { content: llmResponse },
        usage: { inputTokens: 10, outputTokens: 15 },
      });
      generation.end();

      rootSpan.update({ output: { response: llmResponse } });
    }
  );
});
```
</details>

---

### 3.3 æ‰‹åŠ¨åˆ›å»ºè§‚æµ‹

é€‚åˆéœ€è¦å®Œå…¨æ§åˆ¶ span ç”Ÿå‘½å‘¨æœŸçš„åœºæ™¯ã€‚

<details>
<summary>ğŸ Python</summary>

```python
from langfuse import get_client

langfuse = get_client()

# åˆ›å»ºçˆ¶ span
parent = langfuse.start_observation(name="manual-parent")

# åˆ›å»ºå­ span
child_span = parent.start_observation(name="processing-step")
child_span.update(output={"status": "completed"})
child_span.end()

# åˆ›å»ºå­ generation
child_gen = parent.start_observation(
    name="llm-call",
    as_type="generation",
    model="gpt-4"
)
child_gen.update(
    output="LLM å“åº”å†…å®¹",
    usage={"input_tokens": 50, "output_tokens": 100}
)
child_gen.end()

# ç»“æŸçˆ¶ span
parent.update(output={"final_result": "success"})
parent.end()

# ç¡®ä¿æ•°æ®å‘é€å®Œæˆ
langfuse.flush()
```
</details>

<details>
<summary>ğŸ“˜ JavaScript/TypeScript</summary>

```typescript
import { startObservation } from "@langfuse/tracing";
import { sdk } from "./instrumentation";

// åˆ›å»ºçˆ¶ span
const parent = startObservation("manual-parent");

// åˆ›å»ºå­ spanï¼ˆæ‰‹åŠ¨æ–¹å¼éœ€è¦é€šè¿‡å‚æ•°å…³è”ï¼‰
const childSpan = startObservation("processing-step", {}, { asType: "span" });
childSpan.update({ output: { status: "completed" } });
childSpan.end();

// åˆ›å»ºå­ generation
const childGen = startObservation(
  "llm-call",
  { model: "gpt-4" },
  { asType: "generation" }
);
childGen.update({
  output: "LLM å“åº”å†…å®¹",
  usage: { inputTokens: 50, outputTokens: 100 },
});
childGen.end();

// ç»“æŸçˆ¶ span
parent.update({ output: { finalResult: "success" } });
parent.end();

// ç¡®ä¿æ•°æ®å‘é€å®Œæˆï¼ˆçŸ­ç”Ÿå‘½å‘¨æœŸåº”ç”¨éœ€è¦ï¼‰
await sdk.shutdown();
```
</details>

---

## 4. åµŒå¥—è¿½è¸ª

Langfuse è‡ªåŠ¨å¤„ç†è§‚æµ‹çš„å±‚çº§åµŒå¥—å…³ç³»ã€‚

<details>
<summary>ğŸ Python - ä½¿ç”¨è£…é¥°å™¨è‡ªåŠ¨åµŒå¥—</summary>

```python
from langfuse import observe

@observe()
def main_pipeline(user_input: str):
    """ä¸»æµç¨‹ - è‡ªåŠ¨æˆä¸º root span"""
    preprocessed = preprocess(user_input)
    response = generate_response(preprocessed)
    return postprocess(response)

@observe()
def preprocess(text: str):
    """é¢„å¤„ç† - è‡ªåŠ¨æˆä¸º main_pipeline çš„å­ span"""
    return text.strip().lower()

@observe(as_type="generation")
def generate_response(text: str):
    """ç”Ÿæˆå“åº” - è‡ªåŠ¨æˆä¸º main_pipeline çš„å­ generation"""
    return f"AI å“åº”: {text}"

@observe()
def postprocess(response: str):
    """åå¤„ç† - è‡ªåŠ¨æˆä¸º main_pipeline çš„å­ span"""
    return {"response": response, "timestamp": "2024-01-01"}

# è°ƒç”¨æ—¶è‡ªåŠ¨ç”Ÿæˆå®Œæ•´çš„ trace æ ‘
result = main_pipeline("Hello World")
```

**ç”Ÿæˆçš„ Trace ç»“æ„ï¼š**
```
Trace: main_pipeline
â”œâ”€â”€ Span: preprocess
â”œâ”€â”€ Generation: generate_response
â””â”€â”€ Span: postprocess
```
</details>

<details>
<summary>ğŸ“˜ JavaScript/TypeScript - ä½¿ç”¨ startActiveObservation åµŒå¥—</summary>

```typescript
import { startActiveObservation, startObservation } from "@langfuse/tracing";

async function mainPipeline(userInput: string) {
  return await startActiveObservation("main-pipeline", async (rootSpan) => {
    rootSpan.update({ input: { userInput } });

    // å­ span - preprocess
    const preprocessed = await startActiveObservation(
      "preprocess",
      async (span) => {
        const result = userInput.trim().toLowerCase();
        span.update({ output: { result } });
        return result;
      }
    );

    // å­ generation
    const generation = startObservation(
      "generate-response",
      { model: "gpt-4" },
      { asType: "generation" }
    );
    const response = `AI å“åº”: ${preprocessed}`;
    generation.update({ output: { response } });
    generation.end();

    // å­ span - postprocess
    const finalResult = await startActiveObservation(
      "postprocess",
      async (span) => {
        const result = { response, timestamp: new Date().toISOString() };
        span.update({ output: result });
        return result;
      }
    );

    rootSpan.update({ output: finalResult });
    return finalResult;
  });
}

// ä½¿ç”¨
const result = await mainPipeline("Hello World");
```
</details>

---

## 5. å±æ€§ä¸å…ƒæ•°æ®

### å¯ç”¨å±æ€§

| å±æ€§ | è¯´æ˜ | ç”¨é€” |
|------|------|------|
| `user_id` / `userId` | ç”¨æˆ· ID | ç”¨æˆ·çº§åˆ«åˆ†æ |
| `session_id` / `sessionId` | ä¼šè¯ ID | ä¼šè¯çº§åˆ«åˆ†æ |
| `metadata` | å…ƒæ•°æ®å­—å…¸ | è‡ªå®šä¹‰é”®å€¼å¯¹ |
| `version` | åº”ç”¨ç‰ˆæœ¬ | ç‰ˆæœ¬è¿½è¸ª |
| `tags` | æ ‡ç­¾åˆ—è¡¨ | åˆ†ç±»å’Œè¿‡æ»¤ |
| `trace_name` / `traceName` | Trace åç§° | è¦†ç›–é»˜è®¤åç§° |

<details>
<summary>ğŸ Python - ä½¿ç”¨ propagate_attributes</summary>

```python
from langfuse import observe, propagate_attributes

@observe()
def handle_user_request(user_id: str, query: str):
    with propagate_attributes(
        user_id=user_id,
        session_id="session_" + user_id,
        metadata={
            "env": "production",
            "feature_flag": "new_model_v2",
            "source": "web_app"
        },
        version="1.2.0",
        tags=["production", "priority-high"]
    ):
        result = process_query(query)
        return result

@observe()
def process_query(query: str):
    # è‡ªåŠ¨ç»§æ‰¿ user_id, session_id ç­‰å±æ€§
    return {"result": query.upper()}

handle_user_request("user_123", "ä½ å¥½")
```
</details>

<details>
<summary>ğŸ“˜ JavaScript/TypeScript - ä½¿ç”¨ propagateAttributes</summary>

```typescript
import {
  startActiveObservation,
  propagateAttributes,
  startObservation,
} from "@langfuse/tracing";

await startActiveObservation("user-workflow", async () => {
  await propagateAttributes(
    {
      userId: "user_123",
      sessionId: "session_abc",
      metadata: { experiment: "variant_a", env: "prod" },
      version: "1.0",
      traceName: "user-workflow",
    },
    async () => {
      const generation = startObservation(
        "llm-call",
        { model: "gpt-4" },
        { asType: "generation" }
      );
      generation.end();
    }
  );
});
```
</details>

---

## 6. ä¸ OpenAI é›†æˆ

<details>
<summary>ğŸ Python</summary>

### æ–¹å¼ä¸€ï¼šä½¿ç”¨ Langfuse å°è£…çš„ OpenAI å®¢æˆ·ç«¯

```python
from langfuse.openai import openai

# ç›´æ¥ä½¿ç”¨ï¼Œæ‰€æœ‰è°ƒç”¨è‡ªåŠ¨è¿½è¸ª
response = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
    ]
)

print(response.choices[0].message.content)
```

### æ–¹å¼äºŒï¼šåŒ…è£…ç°æœ‰ OpenAI å®¢æˆ·ç«¯

```python
from openai import OpenAI
from langfuse.openai import wrap_openai

client = OpenAI(api_key="your-api-key")
wrapped_client = wrap_openai(client)

response = wrapped_client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "ä½ å¥½"}]
)
```

### åœ¨ @observe ä¸­ä½¿ç”¨

```python
from langfuse import observe
from langfuse.openai import openai

@observe()
def chat_with_user(user_message: str):
    response = openai.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": user_message}
        ],
        temperature=0.7
    )
    return response.choices[0].message.content

result = chat_with_user("ä»‹ç»ä¸€ä¸‹ Python")
```
</details>

<details>
<summary>ğŸ“˜ JavaScript/TypeScript</summary>

### å®‰è£…

```bash
npm install @langfuse/openai openai
```

### ä½¿ç”¨ Langfuse å°è£…çš„ OpenAI å®¢æˆ·ç«¯

```typescript
import { withTracing } from "@langfuse/openai";
import OpenAI from "openai";

const openai = withTracing(new OpenAI({ apiKey: "your-api-key" }));

const response = await openai.chat.completions.create({
  model: "gpt-4",
  messages: [
    { role: "system", content: "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚" },
    { role: "user", content: "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ" },
  ],
});

console.log(response.choices[0].message.content);
```

### åœ¨ startActiveObservation ä¸­ä½¿ç”¨

```typescript
import { startActiveObservation } from "@langfuse/tracing";
import { withTracing } from "@langfuse/openai";
import OpenAI from "openai";

const openai = withTracing(new OpenAI());

await startActiveObservation("chat-with-user", async (span) => {
  span.update({ input: { userMessage: "ä»‹ç»ä¸€ä¸‹ TypeScript" } });

  const response = await openai.chat.completions.create({
    model: "gpt-4",
    messages: [
      { role: "system", content: "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ã€‚" },
      { role: "user", content: "ä»‹ç»ä¸€ä¸‹ TypeScript" },
    ],
    temperature: 0.7,
  });

  const content = response.choices[0].message.content;
  span.update({ output: { response: content } });

  return content;
});
```
</details>

---

## 7. ä¸ LangChain é›†æˆ

<details>
<summary>ğŸ Python</summary>

```python
from langfuse.callback import CallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# åˆ›å»º Langfuse å›è°ƒå¤„ç†å™¨
langfuse_handler = CallbackHandler(
    public_key="pk-lf-xxx",
    secret_key="sk-lf-xxx",
    host="https://cloud.langfuse.com"
)

# åˆ›å»º LangChain ç»„ä»¶
llm = ChatOpenAI(model="gpt-4", temperature=0.7)
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"),
    ("user", "{input}")
])

chain = prompt | llm

# ä½¿ç”¨å›è°ƒè¿½è¸ª
response = chain.invoke(
    {"input": "ä»€ä¹ˆæ˜¯ LangChain?"},
    config={"callbacks": [langfuse_handler]}
)

print(response.content)
```

### è®¾ç½®ç”¨æˆ·å’Œä¼šè¯ä¿¡æ¯

```python
langfuse_handler = CallbackHandler(
    user_id="user_123",
    session_id="session_abc",
    metadata={"source": "chatbot"},
    tags=["langchain", "production"]
)
```
</details>

<details>
<summary>ğŸ“˜ JavaScript/TypeScript</summary>

### å®‰è£…

```bash
npm install @langfuse/langchain @langchain/core @langchain/openai
```

### ä½¿ç”¨

```typescript
import { CallbackHandler } from "@langfuse/langchain";
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";

// åˆ›å»º Langfuse å›è°ƒå¤„ç†å™¨
const langfuseHandler = new CallbackHandler({
  publicKey: "pk-lf-xxx",
  secretKey: "sk-lf-xxx",
  baseUrl: "https://cloud.langfuse.com",
});

// åˆ›å»º LangChain ç»„ä»¶
const llm = new ChatOpenAI({ modelName: "gpt-4", temperature: 0.7 });
const prompt = ChatPromptTemplate.fromMessages([
  ["system", "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"],
  ["user", "{input}"],
]);

const chain = prompt.pipe(llm);

// ä½¿ç”¨å›è°ƒè¿½è¸ª
const response = await chain.invoke(
  { input: "ä»€ä¹ˆæ˜¯ LangChain?" },
  { callbacks: [langfuseHandler] }
);

console.log(response.content);
```

### è®¾ç½®ç”¨æˆ·å’Œä¼šè¯ä¿¡æ¯

```typescript
const langfuseHandler = new CallbackHandler({
  userId: "user_123",
  sessionId: "session_abc",
  metadata: { source: "chatbot" },
  tags: ["langchain", "production"],
});
```
</details>

---

## 8. è¯„åˆ†ä¸åé¦ˆ

<details>
<summary>ğŸ Python</summary>

```python
from langfuse import get_client, get_current_trace_id, observe

langfuse = get_client()

# ç›´æ¥é€šè¿‡ trace_id æ·»åŠ è¯„åˆ†
langfuse.score(
    trace_id="your-trace-id",
    name="user-feedback",
    value=1,
    comment="å›ç­”å¾ˆæœ‰å¸®åŠ©"
)

# æ•°å€¼è¯„åˆ†
langfuse.score(
    trace_id="your-trace-id",
    name="quality-score",
    value=0.85,
    data_type="NUMERIC"
)

# åœ¨è¿½è¸ªä¸­è·å– trace_id
@observe()
def my_function():
    trace_id = get_current_trace_id()
    print(f"Trace ID: {trace_id}")
    return {"trace_id": trace_id, "result": "success"}
```
</details>

<details>
<summary>ğŸ“˜ JavaScript/TypeScript</summary>

```typescript
import { LangfuseClient } from "@langfuse/client";
import {
  startActiveObservation,
  getActiveTraceId,
} from "@langfuse/tracing";

const langfuse = new LangfuseClient();

// ç›´æ¥é€šè¿‡ trace_id æ·»åŠ è¯„åˆ†
await langfuse.score({
  traceId: "your-trace-id",
  name: "user-feedback",
  value: 1,
  comment: "å›ç­”å¾ˆæœ‰å¸®åŠ©",
});

// æ•°å€¼è¯„åˆ†
await langfuse.score({
  traceId: "your-trace-id",
  name: "quality-score",
  value: 0.85,
  dataType: "NUMERIC",
});

// åœ¨è¿½è¸ªä¸­è·å– trace_id
await startActiveObservation("my-function", async (span) => {
  const traceId = getActiveTraceId();
  console.log(`Trace ID: ${traceId}`);

  span.update({ output: { traceId, result: "success" } });
});
```
</details>

---

## 9. å®Œæ•´ç¤ºä¾‹

### RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰åº”ç”¨ç¤ºä¾‹

<details>
<summary>ğŸ Python å®Œæ•´ç¤ºä¾‹</summary>

```python
from langfuse import observe, get_client, propagate_attributes
from langfuse.openai import openai

langfuse = get_client()

# æ¨¡æ‹ŸçŸ¥è¯†åº“
KNOWLEDGE_BASE = {
    "langfuse": "Langfuse æ˜¯ä¸€ä¸ªå¼€æºçš„ LLM å¯è§‚æµ‹æ€§å¹³å°ã€‚",
    "opentelemetry": "OpenTelemetry æ˜¯ä¸€ä¸ªå¼€æºçš„å¯è§‚æµ‹æ€§æ¡†æ¶ã€‚",
}

@observe(name="search-knowledge", as_type="span")
def search_knowledge_base(query: str) -> list:
    """æ¨¡æ‹Ÿå‘é‡æœç´¢"""
    results = []
    for key, value in KNOWLEDGE_BASE.items():
        if key in query.lower():
            results.append({"topic": key, "content": value, "score": 0.9})
    return results

@observe(name="generate-response", as_type="generation")
def generate_response(query: str, context: list) -> str:
    """ä½¿ç”¨ LLM ç”Ÿæˆå“åº”"""
    context_text = "\n".join([f"- {item['content']}" for item in context])

    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": f"åŸºäºä¸Šä¸‹æ–‡å›ç­”ï¼š\n{context_text}"},
            {"role": "user", "content": query}
        ]
    )
    return response.choices[0].message.content

@observe(name="rag-pipeline")
def rag_pipeline(user_id: str, query: str) -> dict:
    """å®Œæ•´çš„ RAG æµç¨‹"""
    with propagate_attributes(
        user_id=user_id,
        metadata={"pipeline": "rag"},
        tags=["rag", "production"]
    ):
        retrieved_docs = search_knowledge_base(query)
        response = generate_response(query, retrieved_docs) if retrieved_docs else "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"

        return {
            "query": query,
            "response": response,
            "sources": [doc["topic"] for doc in retrieved_docs]
        }

if __name__ == "__main__":
    result = rag_pipeline("user_001", "ä»€ä¹ˆæ˜¯ Langfuseï¼Ÿ")
    print(f"é—®é¢˜: {result['query']}")
    print(f"å›ç­”: {result['response']}")
    langfuse.flush()
```
</details>

<details>
<summary>ğŸ“˜ JavaScript/TypeScript å®Œæ•´ç¤ºä¾‹</summary>

```typescript
// instrumentation.ts
import { NodeSDK } from "@opentelemetry/sdk-node";
import { LangfuseSpanProcessor } from "@langfuse/otel";

export const sdk = new NodeSDK({
  spanProcessors: [new LangfuseSpanProcessor()],
});
sdk.start();

// main.ts
import { sdk } from "./instrumentation";
import {
  startActiveObservation,
  startObservation,
  propagateAttributes,
} from "@langfuse/tracing";
import { withTracing } from "@langfuse/openai";
import OpenAI from "openai";

const openai = withTracing(new OpenAI());

// æ¨¡æ‹ŸçŸ¥è¯†åº“
const KNOWLEDGE_BASE: Record<string, string> = {
  langfuse: "Langfuse æ˜¯ä¸€ä¸ªå¼€æºçš„ LLM å¯è§‚æµ‹æ€§å¹³å°ã€‚",
  opentelemetry: "OpenTelemetry æ˜¯ä¸€ä¸ªå¼€æºçš„å¯è§‚æµ‹æ€§æ¡†æ¶ã€‚",
};

async function searchKnowledgeBase(query: string) {
  return await startActiveObservation("search-knowledge", async (span) => {
    const results: Array<{ topic: string; content: string; score: number }> = [];

    for (const [key, value] of Object.entries(KNOWLEDGE_BASE)) {
      if (query.toLowerCase().includes(key)) {
        results.push({ topic: key, content: value, score: 0.9 });
      }
    }

    span.update({ output: { results } });
    return results;
  });
}

async function generateResponse(query: string, context: typeof results) {
  const generation = startObservation(
    "generate-response",
    { model: "gpt-3.5-turbo" },
    { asType: "generation" }
  );

  const contextText = context.map((item) => `- ${item.content}`).join("\n");

  const response = await openai.chat.completions.create({
    model: "gpt-3.5-turbo",
    messages: [
      { role: "system", content: `åŸºäºä¸Šä¸‹æ–‡å›ç­”ï¼š\n${contextText}` },
      { role: "user", content: query },
    ],
  });

  const content = response.choices[0].message.content;
  generation.update({ output: { content } });
  generation.end();

  return content;
}

async function ragPipeline(userId: string, query: string) {
  return await startActiveObservation("rag-pipeline", async (span) => {
    span.update({ input: { userId, query } });

    return await propagateAttributes(
      { userId, metadata: { pipeline: "rag" }, tags: ["rag", "production"] },
      async () => {
        const retrievedDocs = await searchKnowledgeBase(query);
        const response = retrievedDocs.length
          ? await generateResponse(query, retrievedDocs)
          : "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯";

        const result = {
          query,
          response,
          sources: retrievedDocs.map((doc) => doc.topic),
        };

        span.update({ output: result });
        return result;
      }
    );
  });
}

// è¿è¡Œ
async function main() {
  const result = await ragPipeline("user_001", "ä»€ä¹ˆæ˜¯ Langfuseï¼Ÿ");
  console.log(`é—®é¢˜: ${result.query}`);
  console.log(`å›ç­”: ${result.response}`);
}

main().finally(() => sdk.shutdown());
```
</details>

---

## 10. å¸¸è§é—®é¢˜

### Q1: æ•°æ®æ²¡æœ‰å‡ºç°åœ¨ Langfuse æ§åˆ¶å°ï¼Ÿ

<details>
<summary>ğŸ Python</summary>

```python
from langfuse import get_client
langfuse = get_client()
# ... ä½ çš„ä»£ç  ...
langfuse.flush()  # å¼ºåˆ¶å‘é€æ‰€æœ‰å¾…å¤„ç†æ•°æ®
```
</details>

<details>
<summary>ğŸ“˜ JavaScript/TypeScript</summary>

```typescript
import { sdk } from "./instrumentation";
// ... ä½ çš„ä»£ç  ...
await sdk.shutdown(); // å¯¹äºçŸ­ç”Ÿå‘½å‘¨æœŸåº”ç”¨å¿…é¡»è°ƒç”¨
```
</details>

### Q2: å¦‚ä½•åœ¨å¼‚æ­¥ä»£ç ä¸­ä½¿ç”¨ï¼Ÿ

ä¸¤ç§è¯­è¨€çš„ SDK éƒ½åŸç”Ÿæ”¯æŒå¼‚æ­¥æ“ä½œï¼Œè£…é¥°å™¨å’ŒåŒ…è£…å™¨å¯ä»¥ç›´æ¥ç”¨äºå¼‚æ­¥å‡½æ•°ã€‚

### Q3: å¦‚ä½•è°ƒè¯•è¿æ¥é—®é¢˜ï¼Ÿ

<details>
<summary>ğŸ Python</summary>

```python
import logging
logging.basicConfig(level=logging.DEBUG)
# æˆ– export LANGFUSE_DEBUG=true
```
</details>

<details>
<summary>ğŸ“˜ JavaScript/TypeScript</summary>

```typescript
// è®¾ç½®ç¯å¢ƒå˜é‡
// export LANGFUSE_DEBUG=true
```
</details>

### Q4: Python å’Œ JS SDK çš„ä¸»è¦åŒºåˆ«ï¼Ÿ

| ç‰¹æ€§ | Python | JavaScript/TypeScript |
|------|--------|----------------------|
| OpenTelemetry åˆå§‹åŒ– | è‡ªåŠ¨ | éœ€æ‰‹åŠ¨é…ç½® NodeSDK |
| è£…é¥°å™¨è¯­æ³• | `@observe()` | `observe(fn, options)` |
| ä¸Šä¸‹æ–‡ç®¡ç†å™¨ | `with ... as span:` | `await startActiveObservation(name, async (span) => {})` |
| å±æ€§ä¼ æ’­ | `with propagate_attributes():` | `await propagateAttributes({}, async () => {})` |
| ç”Ÿå‘½å‘¨æœŸç®¡ç† | `langfuse.flush()` | `sdk.shutdown()` |

---

## å‚è€ƒé“¾æ¥

- [Langfuse å®˜æ–¹æ–‡æ¡£](https://langfuse.com/docs)
- [Langfuse Python SDK å‚è€ƒ](https://python.reference.langfuse.com)
- [Langfuse JS SDK å‚è€ƒ](https://langfuse-js.vercel.app)
- [OpenTelemetry å®˜æ–¹æ–‡æ¡£](https://opentelemetry.io/docs)
- [Langfuse GitHub](https://github.com/langfuse/langfuse)

---

## è®¸å¯è¯

MIT License